1Ô∏è‚É£ Verify that OIDC Provider is Configured
Since you already enabled OIDC, verify it exists:

bash
Copy
Edit
aws eks describe-cluster --name app-observability --query "cluster.identity.oidc.issuer" --output text
If the output is empty, you need to re-enable OIDC:

bash
Copy
Edit
eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster app-observability --approve
2Ô∏è‚É£ Check if Service Account is Created Correctly
List the service accounts in the kube-system namespace:

bash
Copy
Edit
kubectl get sa -n kube-system | grep ebs-csi-controller-sa
If it does not exist, create it:

bash
Copy
Edit
eksctl create iamserviceaccount \
  --name ebs-csi-controller-sa \
  --namespace kube-system \
  --cluster app-observability \
  --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
  --approve \
  --region ap-south-1
Then restart the EBS CSI pods:

bash
Copy
Edit
kubectl delete pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver
3Ô∏è‚É£ Check if EC2 IMDS is Accessible
Since the error mentions no EC2 IMDS role found, check if IMDS is properly configured:

bash
Copy
Edit
aws ec2 describe-instances --instance-ids <node-instance-id> --query "Reservations[].Instances[].MetadataOptions"
If HttpTokens is set to "required", change it to "optional":

bash
Copy
Edit
aws ec2 modify-instance-metadata-options \
  --instance-id <node-instance-id> \
  --http-tokens optional \
  --region ap-south-1
Then restart the nodes.

4Ô∏è‚É£ Ensure Node IAM Role Has Permissions
Check the IAM role of your worker nodes:

bash
Copy
Edit
aws ec2 describe-instances --filters "Name=tag:eks:nodegroup-name,Values=*" --query "Reservations[].Instances[].IamInstanceProfile.Arn"
Find the role name from the output and attach the EBS CSI policy:

bash
Copy
Edit
aws iam attach-role-policy \
  --role-name <NodeInstanceRole> \
  --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
5Ô∏è‚É£ Restart All CSI Components
bash
Copy
Edit
kubectl delete pod -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver
Then retry creating the pod with the PVC.

Final Check
Once you've done all the steps, try listing PVCs:

bash
Copy
Edit
kubectl get pvc -A
If the PVC is still Pending, describe it:

bash
Copy
Edit
kubectl describe pvc <your-pvc-name> -n monitoring
Let me know what you get after these fixes! üöÄ